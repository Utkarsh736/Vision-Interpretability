# Segment 1 — Intro

This is the very first part of our notebook, and it would be a nice idea to delve into a few basics about CNNs before we dig deeper.

This project focuses on vision interpretability, not on teaching convolutional neural networks (CNNs) from first principles. However, most interpretability techniques implicitly assume that participants share a clear mental model of how CNNs operate. In practice, when this grounding is missing, interpretability results become harder to reason about and easier to misinterpret.

So ideally, this segment should focus on:

- First impressions of data (image) on the CNN model  
- How convolutional filters act as local pattern detectors  
- How spatial locality and weight sharing arise in CNNs  
- How increasing depth leads to progressively more abstract feature representations  

---

## How to explore this in Segment 1 (3–4 code blocks max)

Write your own version of how we can explore these in the first segment, using **no more than 3–4 code blocks in Colab**.

---

## Submission Instructions

Add your notebook to the `submissions/` folder using the following naming format:

github-username__segment_1_intro.ipynb
